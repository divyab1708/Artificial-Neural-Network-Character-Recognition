{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANN-Project-2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGU-F2g5SIOD"
      },
      "source": [
        "Project 2 Group 5\n",
        "\n",
        "Members:\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Name: Divya Barsode \n",
        "\n",
        "Email: divya.barsode@csu.fullerton.edu\n",
        "\n",
        "CWID: 886699289\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Name: Vedant Terkar\n",
        "\n",
        "Email: vedant.terkar@csu.fullerton.edu\n",
        "\n",
        "CWID: 887352839\n",
        "\n",
        "---\n",
        "\n",
        "NOTE : place dataset.py inside the root folder\n",
        "Structure: ./dataset.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDEJNTrcSUFx"
      },
      "source": [
        "**1) Use from dataset import * to load the module, then examine TRAINING_SET, TEST_SET, and MESSAGE.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLhfJoYQSekJ"
      },
      "source": [
        "from dataset import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpkNHIDESidF",
        "outputId": "0fac76f4-aa4c-4157-927c-e66be3488649"
      },
      "source": [
        "print(\"Training Set\", TRAINING_SET)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Set [([0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1], 'A'), ([1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0], 'B'), ([0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0], 'C'), ([1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0], 'D'), ([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1], 'E'), ([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0], 'F'), ([0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0], 'G'), ([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1], 'H'), ([0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0], 'I'), ([0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0], 'J'), ([1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1], 'K'), ([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1], 'L'), ([1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1], 'M'), ([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1], 'N'), ([0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0], 'O'), ([1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0], 'P'), ([0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1], 'Q'), ([1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1], 'R'), ([0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0], 'S'), ([1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0], 'T'), ([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0], 'U'), ([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], 'V'), ([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1], 'W'), ([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1], 'X'), ([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0], 'Y'), ([1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1], 'Z'), ([0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1], 'A'), ([1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0], 'B'), ([0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0], 'C'), ([1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0], 'D'), ([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1], 'E'), ([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0], 'F'), ([0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0], 'G'), ([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1], 'H'), ([0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0], 'I'), ([0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0], 'J'), ([1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1], 'K'), ([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1], 'L'), ([1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1], 'M'), ([1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1], 'N'), ([0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0], 'O'), ([1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0], 'P'), ([0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1], 'Q'), ([1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1], 'R'), ([0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0], 'S'), ([1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0], 'T'), ([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0], 'U'), ([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], 'V'), ([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1], 'W'), ([1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1], 'X'), ([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0], 'Y'), ([1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1], 'Z')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfI6-SffSpKR",
        "outputId": "b135b8cf-0665-492f-dd99-a734899bb6a0"
      },
      "source": [
        "print(\"Test Set\", TEST_SET)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Set [([0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1], 'A'), ([1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0], 'B'), ([0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0], 'C'), ([1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0], 'D'), ([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1], 'E'), ([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0], 'F'), ([0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1], 'G'), ([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1], 'H'), ([0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0], 'I'), ([0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0], 'J'), ([1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1], 'K'), ([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1], 'L'), ([1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1], 'M'), ([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1], 'N'), ([0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0], 'O'), ([1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0], 'P'), ([0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1], 'Q'), ([1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1], 'R'), ([0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0], 'S'), ([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0], 'T'), ([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0], 'U'), ([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], 'V'), ([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0], 'W'), ([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1], 'X'), ([1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0], 'Y'), ([1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1], 'Z')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5S9xdaQ_St2G",
        "outputId": "7227bd32-d786-45c5-a959-9b2d7f050107"
      },
      "source": [
        "print(\"Message\", MESSAGE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Message [[1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0], [0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1], [1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1], [0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0], [1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1], [0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1], [0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1], [1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1], [1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1], [1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1], [0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1], [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1], [1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1], [1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0], [1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1], [1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1], [1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0], [0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0], [1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0], [1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1], [1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0], [1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1], [1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0], [0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1], [0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0], [0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1], [1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbftZGeeX_L8"
      },
      "source": [
        "**2) In order to use the images in TRAINING_SET, TEST_SET, and MESSAGE, convert them into two-dimensional NumPy arrays of feature vectors.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-h64lZNYCXY"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLimWpo1YGZM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce085d71-120e-4078-ad17-464a29f149b4"
      },
      "source": [
        "# Separate Features and Targets From Training Data\n",
        "training_set, training_targets = [], [];\n",
        "for elem in TRAINING_SET:\n",
        "  training_set.append(elem[0])\n",
        "  training_targets.append(elem[1])\n",
        "\n",
        "# Separate Features and Targets From Testing Data\n",
        "testing_set, testing_targets = [], [];\n",
        "for elem in TEST_SET:\n",
        "  testing_set.append(elem[0])\n",
        "  testing_targets.append(elem[1])\n",
        "\n",
        "print(training_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1], [1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0], [0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0], [1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0], [1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1], [0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0], [0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0], [1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1], [1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1], [1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1], [0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0], [1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1], [1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1], [0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0], [1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0], [1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1], [1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1], [1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1], [0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1], [1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0], [0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0], [1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0], [1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1], [0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0], [1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1], [1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1], [1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1], [0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0], [1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0], [0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1], [1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1], [0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0], [1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0], [1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0], [1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0], [1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1], [1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1], [1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZMq_6pH8saI",
        "outputId": "b7d1cb29-8eae-4aad-9fac-8e49ee3740ce"
      },
      "source": [
        "x_train = np.array(training_set)\n",
        "x_train, x_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0, 1, 1, ..., 0, 0, 1],\n",
              "        [1, 1, 1, ..., 1, 1, 0],\n",
              "        [0, 1, 1, ..., 1, 1, 0],\n",
              "        ...,\n",
              "        [1, 0, 0, ..., 0, 0, 1],\n",
              "        [1, 0, 0, ..., 1, 0, 0],\n",
              "        [1, 1, 1, ..., 1, 1, 1]]), (52, 35))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrnEvWzWBs71",
        "outputId": "3848b089-c900-498f-a956-a9eebe3aa1f5"
      },
      "source": [
        "x_test = np.array(testing_set)\n",
        "x_test[10], x_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
              "        1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1]), (26, 35))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "VVfhkCZnaZdY",
        "outputId": "de478600-73fb-4f45-c551-5e5e0b1be9a0"
      },
      "source": [
        "msg = np.array(MESSAGE)\n",
        "msg , msg.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b5c1702d7193>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoJyCAh4cWje"
      },
      "source": [
        "**3. In order to use the character labels in TRAINING_SET and TEST_SET, convert them into an integer class vector using ord(), then into 26 one-hot encoded categorical features.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07oAh0u5gtRG"
      },
      "source": [
        "**convert them into an integer class vector using ord()**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEId5uALcY3N",
        "outputId": "0a308bf6-e6e3-4a5f-c759-e8933c94a42f"
      },
      "source": [
        "training_targets_ord = []\n",
        "for x in training_targets:\n",
        "  training_targets_ord.append(ord(x) - 65)\n",
        "\n",
        "testing_targets_ord = []\n",
        "for x in testing_targets:\n",
        "  testing_targets_ord.append(ord(x) - 65)\n",
        "\n",
        "training_targets_ord[:5], testing_targets_ord[5:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0, 1, 2, 3, 4], [5, 6, 7, 8, 9])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "068qUQblg0dF"
      },
      "source": [
        "**into 26 one-hot encoded categorical features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoPxUeCHc9An",
        "outputId": "48bfe9b6-8b94-49bc-b661-afde89fead09"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "y_train = tf.keras.utils.to_categorical(training_targets_ord)\n",
        "len(training_targets_ord), len(y_train[0]), y_train[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(52,\n",
              " 26,\n",
              " array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "ZUArIQx6f-oJ",
        "outputId": "1332c3b6-98e1-49dc-8922-d3d826958740"
      },
      "source": [
        "y_test = tf.keras.utils.to_categorical(testing_targets_ord)\n",
        "len(testing_targets_ord), len(y_test[0])\n",
        "y_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-6f0a68a69166>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_targets_ord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_targets_ord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAzI8wRPhBsc"
      },
      "source": [
        "**4. Create a Sequential Keras model with a Dense hidden layer and a Dense output layer with softmax activation.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9b-pQb4hH2q"
      },
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(35,input_shape=(35,)))\n",
        "model.add(tf.keras.layers.Dense(8))\n",
        "model.add(tf.keras.layers.Dense(26, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfRfRXWBn6xK"
      },
      "source": [
        "**5. compile and fit the model to the training set. Train the model until the accuracy is as high as possible. You may wish to use an EarlyStopping callback.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-5_dOuIpV24",
        "outputId": "dadabbc1-1a3f-4674-bf4d-dc1875e7ea82"
      },
      "source": [
        "EPOCHS, PATIENCE = 350, 30\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=PATIENCE)\n",
        "history = model.fit(x_train, y_train, batch_size=1, epochs=EPOCHS, callbacks=[callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/350\n",
            "52/52 [==============================] - 1s 1ms/step - loss: 0.0379 - accuracy: 0.0059\n",
            "Epoch 2/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0368 - accuracy: 0.1288\n",
            "Epoch 3/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0368 - accuracy: 0.0712\n",
            "Epoch 4/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0357 - accuracy: 0.1266\n",
            "Epoch 5/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0354 - accuracy: 0.1930\n",
            "Epoch 6/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0319 - accuracy: 0.3029\n",
            "Epoch 7/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0320 - accuracy: 0.2602\n",
            "Epoch 8/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0323 - accuracy: 0.2892\n",
            "Epoch 9/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0297 - accuracy: 0.2862\n",
            "Epoch 10/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0238 - accuracy: 0.4768\n",
            "Epoch 11/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0239 - accuracy: 0.5896\n",
            "Epoch 12/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0264 - accuracy: 0.3934\n",
            "Epoch 13/350\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.0229 - accuracy: 0.5352\n",
            "Epoch 14/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0206 - accuracy: 0.5856\n",
            "Epoch 15/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0218 - accuracy: 0.5431\n",
            "Epoch 16/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0211 - accuracy: 0.5110\n",
            "Epoch 17/350\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.0165 - accuracy: 0.6958\n",
            "Epoch 18/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0190 - accuracy: 0.7015\n",
            "Epoch 19/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0163 - accuracy: 0.7492\n",
            "Epoch 20/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0126 - accuracy: 0.7991\n",
            "Epoch 21/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0163 - accuracy: 0.6799\n",
            "Epoch 22/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0102 - accuracy: 0.8381\n",
            "Epoch 23/350\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.0132 - accuracy: 0.7991\n",
            "Epoch 24/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0123 - accuracy: 0.8714\n",
            "Epoch 25/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0079 - accuracy: 0.9165\n",
            "Epoch 26/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0065 - accuracy: 0.9251\n",
            "Epoch 27/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0101 - accuracy: 0.8421\n",
            "Epoch 28/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0068 - accuracy: 0.9440\n",
            "Epoch 29/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0112 - accuracy: 0.7514\n",
            "Epoch 30/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0086 - accuracy: 0.9010\n",
            "Epoch 31/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0053 - accuracy: 0.9408\n",
            "Epoch 32/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0052 - accuracy: 0.9319\n",
            "Epoch 33/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0060 - accuracy: 0.8894\n",
            "Epoch 34/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0053 - accuracy: 0.9409\n",
            "Epoch 35/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0049 - accuracy: 0.9245\n",
            "Epoch 36/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0024 - accuracy: 0.9947\n",
            "Epoch 37/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0040 - accuracy: 0.9703\n",
            "Epoch 38/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0050 - accuracy: 0.9423\n",
            "Epoch 39/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0033 - accuracy: 0.9933\n",
            "Epoch 40/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0035 - accuracy: 0.9845\n",
            "Epoch 41/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0041 - accuracy: 0.9533\n",
            "Epoch 42/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0031 - accuracy: 0.9674\n",
            "Epoch 43/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0015 - accuracy: 0.9938\n",
            "Epoch 44/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0034 - accuracy: 0.9693\n",
            "Epoch 45/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0019 - accuracy: 0.9867\n",
            "Epoch 46/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0012 - accuracy: 0.9912\n",
            "Epoch 47/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0017 - accuracy: 0.9845\n",
            "Epoch 48/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0015 - accuracy: 0.9906\n",
            "Epoch 49/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0017 - accuracy: 0.9810\n",
            "Epoch 50/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0028 - accuracy: 0.9533\n",
            "Epoch 51/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 8.6085e-04 - accuracy: 0.9917\n",
            "Epoch 52/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 9.4544e-04 - accuracy: 0.9917\n",
            "Epoch 53/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0022 - accuracy: 0.9674\n",
            "Epoch 54/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0026 - accuracy: 0.9571\n",
            "Epoch 55/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 6.0081e-04 - accuracy: 0.9943\n",
            "Epoch 56/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0022 - accuracy: 0.9602\n",
            "Epoch 57/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 9.4488e-04 - accuracy: 0.9881\n",
            "Epoch 58/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 5.9594e-04 - accuracy: 0.9933\n",
            "Epoch 59/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0012 - accuracy: 0.9810\n",
            "Epoch 60/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 5.7156e-04 - accuracy: 0.9928\n",
            "Epoch 61/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 9.4628e-04 - accuracy: 0.9860\n",
            "Epoch 62/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 8.2556e-04 - accuracy: 0.9888\n",
            "Epoch 63/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 4.4352e-04 - accuracy: 0.9961\n",
            "Epoch 64/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 8.0449e-04 - accuracy: 0.9860\n",
            "Epoch 65/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 6.4878e-04 - accuracy: 0.9906\n",
            "Epoch 66/350\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 7.4828e-04 - accuracy: 0.9888\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWFQR_0D4dli"
      },
      "source": [
        "**6. evaluate the model on TEST_SET. What accuracy do you obtain? If the accuracy is less than 100%, which test images are misclassified?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2e_0Fu0oaFi",
        "outputId": "34cb6cba-9118-43a4-91ff-8815956b7d1e"
      },
      "source": [
        "dct = model.evaluate(x_test, y_test, return_dict=True)\n",
        "dct[\"accuracy\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_test_function.<locals>.test_function at 0x7f6988c9def0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.0050 - accuracy: 0.9615\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9615384340286255"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGbB3OeMh7JQ"
      },
      "source": [
        "**If the accuracy is less than 100%, which test images are misclassified?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEhdtMxvh6f2",
        "outputId": "ed00484c-66f5-4a2a-92ed-a12376708042"
      },
      "source": [
        "if dct[\"accuracy\"] < 1.00:\n",
        "  p = model.predict(x_test)\n",
        "  sb, idx = [], 0\n",
        "  for y_hat in p:\n",
        "    max_index, max_elem, index = 0, y_hat[0], 0\n",
        "    for elem in y_hat:\n",
        "        if elem > max_elem:\n",
        "          max_index = index\n",
        "          max_elem = elem\n",
        "        index += 1\n",
        "    if chr(65 + idx) != chr(65 + max_index):\n",
        "      sb.append(\"+++++++++++ Wrongly Predicted +++++++++++\\n\")\n",
        "      sb.append(chr(65 + idx)+\" is Predicted as: \"+chr(65+max_index)+\"\\n\")\n",
        "      sb.append(\"+++++++++++ Wrongly Predicted Ends +++++++++++\\n\")\n",
        "    idx +=1 \n",
        "  print(\"\".join(sb)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f698cd2fc20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "+++++++++++ Wrongly Predicted +++++++++++\n",
            "W is Predicted as: U\n",
            "+++++++++++ Wrongly Predicted Ends +++++++++++\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6_1I-Zj5QZB"
      },
      "source": [
        "**7. Use your trained model and chr() to identify the letters in MESSAGE. What does it say in English? (Note that there are no spaces between words.) Why do you suppose this was chosen as the message?**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jJ6XtOj4n3p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1604ae34-7d98-474a-f802-cc3631b57f36"
      },
      "source": [
        "def calculateAccuracyOfPrediction(predicted_msg):\n",
        "  msg_orig  = \"WATCHJEOPARDYALEXTREBEKSFUNTVQUIZGAME\"\n",
        "  count, errors, sb = 0, 0, []\n",
        "  for x in range(len(predicted_msg)):\n",
        "    if predicted_msg[x] == msg_orig[x]:\n",
        "      count += 1\n",
        "      sb.append(predicted_msg[x])\n",
        "    else:\n",
        "      errors += 1\n",
        "      sb.append(\"***\"+predicted_msg[x]+\"***\")\n",
        "\n",
        "  print(\"Message Was Predicted with:\", (count/len(msg_orig)) * 100,\"% Accuracy, However, \", errors, \" Letters were misclassified\")\n",
        "  print(\"Prediction: \", predicted_msg)\n",
        "  print(\"Original: \", msg_orig)\n",
        "  #print(\"Difference: \")\n",
        "  #printmd(\"\".join(sb))\n",
        "\n",
        "def generateReadableMsgFromPrediction(pred):\n",
        "  sb = []\n",
        "  for y_hat in pred:\n",
        "    max_index, max_elem, index = 0, y_hat[0], 0\n",
        "    for elem in y_hat:\n",
        "        if elem > max_elem:\n",
        "          max_index = index\n",
        "          max_elem = elem\n",
        "        index += 1\n",
        "    sb.append(chr(65+max_index))\n",
        "  return \"\".join(sb)\n",
        "\n",
        "\n",
        "pred = model.predict(msg)\n",
        "predicted_msg = generateReadableMsgFromPrediction(pred)\n",
        "calculateAccuracyOfPrediction(predicted_msg)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Message Was Predicted with: 91.8918918918919 % Accuracy, However,  3  Letters were misclassified\n",
            "Prediction:  UATCHJEOPARDYWLEXTREBEKSPUNTVQUIZGAME\n",
            "Original:  WATCHJEOPARDYALEXTREBEKSFUNTVQUIZGAME\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l24vTs4yk8hZ"
      },
      "source": [
        "**Why do you suppose this was chosen as the message?**\n",
        "\n",
        "-> We think it was chosen to show efficient working of multilayer neural networks when noisy unseen data is given"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMC1SQNblTQr"
      },
      "source": [
        "**If you completed Project 1, how does this model compare with the performance of your perceptron models? Were any letters misclassified?**\n",
        "\n",
        "-> This model definitely outperforms the perceptron model, There were very few missclassified letters, We think this is because of adding hidden layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiPVKfEUmoj3"
      },
      "source": [
        "**8. All of the letters in MESSAGE were likely not decoded correctly, so let’s try to improve the performance of the model by adding additional hidden layers. Add two additional hidden layers of the same size as your original hidden layer, then repeat experiments (5) and (7). Does the performance improve?**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAeui2xj5WwX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1003bcea-3e0e-4133-8c64-3f43da106147"
      },
      "source": [
        "## Add two additional hidden layers of the same size as your original hidden layer\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(35,input_shape=(35,)))\n",
        "model.add(tf.keras.layers.Dense(8))\n",
        "model.add(tf.keras.layers.Dense(8)) # 1st extra layer\n",
        "model.add(tf.keras.layers.Dense(8)) # 2nd extra layer\n",
        "model.add(tf.keras.layers.Dense(26, activation='softmax'))\n",
        "\n",
        "## repeat experiment (5)\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=PATIENCE)\n",
        "history = model.fit(x_train, y_train, batch_size=1, epochs=EPOCHS, callbacks=[callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0373 - accuracy: 0.1633\n",
            "Epoch 2/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0371 - accuracy: 0.0834\n",
            "Epoch 3/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0353 - accuracy: 0.1816\n",
            "Epoch 4/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0350 - accuracy: 0.1994\n",
            "Epoch 5/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0355 - accuracy: 0.1116\n",
            "Epoch 6/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0311 - accuracy: 0.2767\n",
            "Epoch 7/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0317 - accuracy: 0.3410\n",
            "Epoch 8/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0303 - accuracy: 0.3608\n",
            "Epoch 9/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0291 - accuracy: 0.3465\n",
            "Epoch 10/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0303 - accuracy: 0.2415\n",
            "Epoch 11/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0283 - accuracy: 0.2808\n",
            "Epoch 12/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0244 - accuracy: 0.5415\n",
            "Epoch 13/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0241 - accuracy: 0.4467\n",
            "Epoch 14/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0220 - accuracy: 0.5151\n",
            "Epoch 15/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0192 - accuracy: 0.5880\n",
            "Epoch 16/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0220 - accuracy: 0.5966\n",
            "Epoch 17/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0200 - accuracy: 0.5962\n",
            "Epoch 18/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0196 - accuracy: 0.5722\n",
            "Epoch 19/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0195 - accuracy: 0.5471\n",
            "Epoch 20/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0137 - accuracy: 0.7832\n",
            "Epoch 21/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0112 - accuracy: 0.8336\n",
            "Epoch 22/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0106 - accuracy: 0.8703\n",
            "Epoch 23/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0113 - accuracy: 0.8450\n",
            "Epoch 24/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0074 - accuracy: 0.9301\n",
            "Epoch 25/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0070 - accuracy: 0.9674\n",
            "Epoch 26/350\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.0035 - accuracy: 0.9632\n",
            "Epoch 27/350\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.0054 - accuracy: 0.9472\n",
            "Epoch 28/350\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.0034 - accuracy: 0.9697\n",
            "Epoch 29/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0066 - accuracy: 0.9015\n",
            "Epoch 30/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0061 - accuracy: 0.8918\n",
            "Epoch 31/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0039 - accuracy: 0.9407\n",
            "Epoch 32/350\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.0021 - accuracy: 0.9679\n",
            "Epoch 33/350\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 0.9743\n",
            "Epoch 34/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0012 - accuracy: 0.9826\n",
            "Epoch 35/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0021 - accuracy: 0.9594\n",
            "Epoch 36/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0026 - accuracy: 0.9448\n",
            "Epoch 37/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0016 - accuracy: 0.9845\n",
            "Epoch 38/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0017 - accuracy: 0.9819\n",
            "Epoch 39/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 5.4783e-04 - accuracy: 1.0000\n",
            "Epoch 40/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0011 - accuracy: 0.9693\n",
            "Epoch 41/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 5.0807e-04 - accuracy: 1.0000\n",
            "Epoch 42/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 5.3874e-04 - accuracy: 1.0000\n",
            "Epoch 43/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 2.6543e-04 - accuracy: 1.0000\n",
            "Epoch 44/350\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 2.0803e-04 - accuracy: 1.0000\n",
            "Epoch 45/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 2.6396e-04 - accuracy: 1.0000\n",
            "Epoch 46/350\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 1.4011e-04 - accuracy: 1.0000\n",
            "Epoch 47/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 1.4802e-04 - accuracy: 1.0000\n",
            "Epoch 48/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 1.3996e-04 - accuracy: 1.0000\n",
            "Epoch 49/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 9.1281e-05 - accuracy: 1.0000\n",
            "Epoch 50/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 1.0078e-04 - accuracy: 1.0000\n",
            "Epoch 51/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 1.3669e-04 - accuracy: 1.0000\n",
            "Epoch 52/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 8.0697e-05 - accuracy: 1.0000\n",
            "Epoch 53/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 8.6205e-05 - accuracy: 1.0000\n",
            "Epoch 54/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 5.8065e-05 - accuracy: 1.0000\n",
            "Epoch 55/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 6.0574e-05 - accuracy: 1.0000\n",
            "Epoch 56/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 5.8952e-05 - accuracy: 1.0000\n",
            "Epoch 57/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 7.3995e-05 - accuracy: 1.0000\n",
            "Epoch 58/350\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 6.4879e-05 - accuracy: 1.0000\n",
            "Epoch 59/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 4.5828e-05 - accuracy: 1.0000\n",
            "Epoch 60/350\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 5.7026e-05 - accuracy: 1.0000\n",
            "Epoch 61/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 3.5807e-05 - accuracy: 1.0000\n",
            "Epoch 62/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 4.7247e-05 - accuracy: 1.0000\n",
            "Epoch 63/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 4.3360e-05 - accuracy: 1.0000\n",
            "Epoch 64/350\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 3.4726e-05 - accuracy: 1.0000\n",
            "Epoch 65/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 3.1601e-05 - accuracy: 1.0000\n",
            "Epoch 66/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 3.9873e-05 - accuracy: 1.0000\n",
            "Epoch 67/350\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 3.1214e-05 - accuracy: 1.0000\n",
            "Epoch 68/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 2.5816e-05 - accuracy: 1.0000\n",
            "Epoch 69/350\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 3.5343e-05 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1e1_ELmq-oO",
        "outputId": "a43f88aa-dae9-46fa-f937-6420240ac706"
      },
      "source": [
        "## repeat experiment (7)\n",
        "pred = model.predict(msg)\n",
        "predicted_msg = generateReadableMsgFromPrediction(pred)\n",
        "calculateAccuracyOfPrediction(predicted_msg)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f6988c9d950> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Message Was Predicted with: 86.48648648648648 % Accuracy, However,  5  Letters were misclassified\n",
            "Prediction:  VATCFJEOPARDYKLEXTREBEKSFUNTVQUIEGRME\n",
            "Original:  WATCHJEOPARDYALEXTREBEKSFUNTVQUIZGAME\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evxOHtrw5bzO"
      },
      "source": [
        "**Does the performance improve?**\n",
        "\n",
        "-> No. It misclassified more letters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drVYNR8HsSOM"
      },
      "source": [
        "**9. Repeat experiment (8), adding additional layers of the same size until the message is decoded correctly. What results do you observe?**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3JkToyUrqg6",
        "outputId": "cf0293f0-9b68-44bd-fbcd-7d07ab210da7"
      },
      "source": [
        "### 3 Hidden Layers\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(35,input_shape=(35,)))\n",
        "model.add(tf.keras.layers.Dense(8))\n",
        "model.add(tf.keras.layers.Dense(8)) # 1st extra layer\n",
        "model.add(tf.keras.layers.Dense(8)) # 2nd extra layer\n",
        "model.add(tf.keras.layers.Dense(8)) # 3rd extra layer\n",
        "model.add(tf.keras.layers.Dense(26, activation='softmax'))\n",
        "\n",
        "## repeat experiment (5)\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=PATIENCE)\n",
        "history = model.fit(x_train, y_train, batch_size=1, epochs=EPOCHS, callbacks=[callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/350\n",
            "52/52 [==============================] - 1s 1ms/step - loss: 0.0374 - accuracy: 0.0053\n",
            "Epoch 2/350\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.0365 - accuracy: 0.0993\n",
            "Epoch 3/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0360 - accuracy: 0.1385\n",
            "Epoch 4/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0352 - accuracy: 0.2181\n",
            "Epoch 5/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0331 - accuracy: 0.2656\n",
            "Epoch 6/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0315 - accuracy: 0.3540\n",
            "Epoch 7/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0303 - accuracy: 0.4204\n",
            "Epoch 8/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0275 - accuracy: 0.3544\n",
            "Epoch 9/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0250 - accuracy: 0.4255\n",
            "Epoch 10/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0231 - accuracy: 0.5171\n",
            "Epoch 11/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0184 - accuracy: 0.6342\n",
            "Epoch 12/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0226 - accuracy: 0.5639\n",
            "Epoch 13/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0198 - accuracy: 0.5742\n",
            "Epoch 14/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0199 - accuracy: 0.6010\n",
            "Epoch 15/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0165 - accuracy: 0.6362\n",
            "Epoch 16/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0129 - accuracy: 0.7342\n",
            "Epoch 17/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0156 - accuracy: 0.7427\n",
            "Epoch 18/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0089 - accuracy: 0.8611\n",
            "Epoch 19/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0085 - accuracy: 0.8689\n",
            "Epoch 20/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0128 - accuracy: 0.7918\n",
            "Epoch 21/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0077 - accuracy: 0.8849\n",
            "Epoch 22/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0091 - accuracy: 0.8265\n",
            "Epoch 23/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0074 - accuracy: 0.8610\n",
            "Epoch 24/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0067 - accuracy: 0.8470\n",
            "Epoch 25/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0046 - accuracy: 0.9241\n",
            "Epoch 26/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0044 - accuracy: 0.9232\n",
            "Epoch 27/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0058 - accuracy: 0.9135\n",
            "Epoch 28/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0049 - accuracy: 0.8968\n",
            "Epoch 29/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0026 - accuracy: 0.9565\n",
            "Epoch 30/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0039 - accuracy: 0.9660\n",
            "Epoch 31/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0032 - accuracy: 0.9906\n",
            "Epoch 32/350\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 0.9925\n",
            "Epoch 33/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0023 - accuracy: 0.9563\n",
            "Epoch 34/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0024 - accuracy: 0.9726\n",
            "Epoch 35/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 8.5859e-04 - accuracy: 0.9933\n",
            "Epoch 36/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 5.2632e-04 - accuracy: 0.9965\n",
            "Epoch 37/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 5.9304e-04 - accuracy: 0.9961\n",
            "Epoch 38/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 4.8357e-04 - accuracy: 0.9957\n",
            "Epoch 39/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 5.0731e-04 - accuracy: 0.9947\n",
            "Epoch 40/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 7.8902e-04 - accuracy: 0.9888\n",
            "Epoch 41/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 5.5617e-04 - accuracy: 0.9912\n",
            "Epoch 42/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0011 - accuracy: 0.9800\n",
            "Epoch 43/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0012 - accuracy: 0.9766\n",
            "Epoch 44/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 2.3888e-04 - accuracy: 0.9977\n",
            "Epoch 45/350\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 8.4641e-04 - accuracy: 0.9845\n",
            "Epoch 46/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 8.0628e-04 - accuracy: 0.9845\n",
            "Epoch 47/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0015 - accuracy: 0.9693\n",
            "Epoch 48/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 6.3707e-04 - accuracy: 0.9881\n",
            "Epoch 49/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 3.5266e-04 - accuracy: 0.9947\n",
            "Epoch 50/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 3.9181e-04 - accuracy: 0.9933\n",
            "Epoch 51/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 1.0718e-04 - accuracy: 0.9989\n",
            "Epoch 52/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 4.9359e-04 - accuracy: 0.9906\n",
            "Epoch 53/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 4.6900e-04 - accuracy: 0.9917\n",
            "Epoch 54/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 3.4482e-04 - accuracy: 0.9943\n",
            "Epoch 55/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 8.5699e-04 - accuracy: 0.9828\n",
            "Epoch 56/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 5.1775e-04 - accuracy: 0.9900\n",
            "Epoch 57/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0011 - accuracy: 0.9766\n",
            "Epoch 58/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 4.2176e-04 - accuracy: 0.9917\n",
            "Epoch 59/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 7.6144e-04 - accuracy: 0.9853\n",
            "Epoch 60/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 8.9644e-04 - accuracy: 0.9828\n",
            "Epoch 61/350\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 0.9653\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uXrhXrX4Kmn",
        "outputId": "c4f05072-079b-4b95-af4b-08cdb7c996e9"
      },
      "source": [
        "print(\"With 3 Hidden Layers\")\n",
        "pred = model.predict(msg)\n",
        "predicted_msg = generateReadableMsgFromPrediction(pred)\n",
        "calculateAccuracyOfPrediction(predicted_msg)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With 3 Hidden Layers\n",
            "WARNING:tensorflow:6 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f698d583dd0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Message Was Predicted with: 72.97297297297297 % Accuracy, However,  10  Letters were misclassified\n",
            "Prediction:  MBTCHJEUPRRDYALLXTPESEKSFUNTVQUOXGNME\n",
            "Original:  WATCHJEOPARDYALEXTREBEKSFUNTVQUIZGAME\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTT0Z5Xd3oVI",
        "outputId": "8a710c1e-1636-429a-b00d-2e9eed4f36af"
      },
      "source": [
        "### 4 Hidden Layers\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(35,input_shape=(35,)))\n",
        "model.add(tf.keras.layers.Dense(8))\n",
        "model.add(tf.keras.layers.Dense(8)) # 1st extra layer\n",
        "model.add(tf.keras.layers.Dense(8)) # 2nd extra layer\n",
        "model.add(tf.keras.layers.Dense(8)) # 3rd extra layer\n",
        "model.add(tf.keras.layers.Dense(8)) # 4th extra layer\n",
        "model.add(tf.keras.layers.Dense(26, activation='softmax'))\n",
        "\n",
        "## repeat experiment (5)\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=PATIENCE)\n",
        "history = model.fit(x_train, y_train, batch_size=1, epochs=EPOCHS, callbacks=[callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/350\n",
            "52/52 [==============================] - 1s 2ms/step - loss: 0.0369 - accuracy: 0.0048\n",
            "Epoch 2/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0367 - accuracy: 0.0925\n",
            "Epoch 3/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0362 - accuracy: 0.1696\n",
            "Epoch 4/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0356 - accuracy: 0.1226\n",
            "Epoch 5/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0348 - accuracy: 0.1712\n",
            "Epoch 6/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0329 - accuracy: 0.3077\n",
            "Epoch 7/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0309 - accuracy: 0.2493\n",
            "Epoch 8/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0282 - accuracy: 0.3897\n",
            "Epoch 9/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0286 - accuracy: 0.3622\n",
            "Epoch 10/350\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.0231 - accuracy: 0.5536\n",
            "Epoch 11/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0263 - accuracy: 0.4591\n",
            "Epoch 12/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0233 - accuracy: 0.5209\n",
            "Epoch 13/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0221 - accuracy: 0.6448\n",
            "Epoch 14/350\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.0202 - accuracy: 0.6058\n",
            "Epoch 15/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0168 - accuracy: 0.6334\n",
            "Epoch 16/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0157 - accuracy: 0.7348\n",
            "Epoch 17/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0128 - accuracy: 0.8326\n",
            "Epoch 18/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0097 - accuracy: 0.8191\n",
            "Epoch 19/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0096 - accuracy: 0.8290\n",
            "Epoch 20/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0128 - accuracy: 0.7898\n",
            "Epoch 21/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0047 - accuracy: 0.9390\n",
            "Epoch 22/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0112 - accuracy: 0.7841\n",
            "Epoch 23/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0106 - accuracy: 0.8005\n",
            "Epoch 24/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0047 - accuracy: 0.9050\n",
            "Epoch 25/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0061 - accuracy: 0.8613\n",
            "Epoch 26/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0039 - accuracy: 0.9220\n",
            "Epoch 27/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0071 - accuracy: 0.8567\n",
            "Epoch 28/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0064 - accuracy: 0.8775\n",
            "Epoch 29/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0040 - accuracy: 0.9242\n",
            "Epoch 30/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0036 - accuracy: 0.9307\n",
            "Epoch 31/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0045 - accuracy: 0.9057\n",
            "Epoch 32/350\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 0.9054\n",
            "Epoch 33/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0033 - accuracy: 0.9343\n",
            "Epoch 34/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0037 - accuracy: 0.9385\n",
            "Epoch 35/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0086 - accuracy: 0.8561\n",
            "Epoch 36/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0094 - accuracy: 0.8197\n",
            "Epoch 37/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0037 - accuracy: 0.9270\n",
            "Epoch 38/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0074 - accuracy: 0.8487\n",
            "Epoch 39/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0030 - accuracy: 0.9403\n",
            "Epoch 40/350\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.0025 - accuracy: 0.9490\n",
            "Epoch 41/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0063 - accuracy: 0.8571\n",
            "Epoch 42/350\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.0026 - accuracy: 0.9395\n",
            "Epoch 43/350\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.0035 - accuracy: 0.9286\n",
            "Epoch 44/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0010 - accuracy: 0.9790\n",
            "Epoch 45/350\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 0.9715\n",
            "Epoch 46/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0039 - accuracy: 0.9196\n",
            "Epoch 47/350\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 0.9773\n",
            "Epoch 48/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 7.8736e-04 - accuracy: 0.9839\n",
            "Epoch 49/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0017 - accuracy: 0.9621\n",
            "Epoch 50/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0034 - accuracy: 0.9229\n",
            "Epoch 51/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 9.5911e-04 - accuracy: 0.9805\n",
            "Epoch 52/350\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 9.1981e-04 - accuracy: 0.9809\n",
            "Epoch 53/350\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 0.9717\n",
            "Epoch 54/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 3.0454e-04 - accuracy: 0.9947\n",
            "Epoch 55/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0045 - accuracy: 0.9057\n",
            "Epoch 56/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0019 - accuracy: 0.9611\n",
            "Epoch 57/350\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.0027 - accuracy: 0.9456\n",
            "Epoch 58/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0012 - accuracy: 0.9737\n",
            "Epoch 59/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 1.9256e-04 - accuracy: 0.9967\n",
            "Epoch 60/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 5.7148e-04 - accuracy: 0.9885\n",
            "Epoch 61/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 8.3926e-04 - accuracy: 0.9819\n",
            "Epoch 62/350\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 0.9720\n",
            "Epoch 63/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 4.9101e-04 - accuracy: 0.9906\n",
            "Epoch 64/350\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.0034 - accuracy: 0.9323\n",
            "Epoch 65/350\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.0049 - accuracy: 0.8900\n",
            "Epoch 66/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0019 - accuracy: 0.9554\n",
            "Epoch 67/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 3.1781e-04 - accuracy: 0.9938\n",
            "Epoch 68/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0022 - accuracy: 0.9555\n",
            "Epoch 69/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0050 - accuracy: 0.8959\n",
            "Epoch 70/350\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 0.9569\n",
            "Epoch 71/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0047 - accuracy: 0.9211\n",
            "Epoch 72/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0157 - accuracy: 0.7492\n",
            "Epoch 73/350\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.0055 - accuracy: 0.9202\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlyVexVi4Qt7",
        "outputId": "85d5721c-7d88-48a0-cea4-574ca03b3b20"
      },
      "source": [
        "print(\"With 4 Hidden Layers\")\n",
        "pred = model.predict(msg)\n",
        "predicted_msg = generateReadableMsgFromPrediction(pred)\n",
        "calculateAccuracyOfPrediction(predicted_msg)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "With 4 Hidden Layers\n",
            "WARNING:tensorflow:6 out of the last 12 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f6986088b00> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "Message Was Predicted with: 75.67567567567568 % Accuracy, However,  9  Letters were misclassified\n",
            "Prediction:  YGTCHJEGPQHOYALEXTYEBEKSFUNTVQUIZGQAE\n",
            "Original:  WATCHJEOPARDYALEXTREBEKSFUNTVQUIZGAME\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4PhQDaC4-w0"
      },
      "source": [
        "**What results do you observe?**\n",
        "\n",
        "-> Adding extra hidden layers doesn't helps much beyond a certain limit. We think model overfits and loses its ability to generalize on unseen data. It missclassifies more letters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-h_jZn95P12"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}